## What & Why

**Decentralized-competitive-community** is a community-run, *zero-storage* leaderboard for open-source LLM and LoRA checkpoints on Hugging Face Hub.  
The guiding principles are:

| Principle | Rationale & Implementation |
|-----------|----------------------------|
| **ğŸŒ Radical Piggy-backing** | All heavy assets (models, branches, logs) live on *external* platformsâ€”mostly HF Hub & GitHub. Our DB only stores immutable URLs + numeric scores, keeping infra bills and lock-in near zero. |
| **ğŸ”¬ Micro-benchmarks, Hourly** | We evaluate *5 shots* of **TruthfulQA** and **GSM8K** every hour. Itâ€™s tiny enough for hobby GPUs yet fresh enough to track rapid LoRA iterations. |
| **ğŸ”’ Tamper-proof by Design** | Each run is anonymously cross-checked by â‰¥ 3 independent runners. Median wins if dispersion â‰¤ 0.5; else the task is flagged **DISPUTED**. Datasets are pinned by git-SHA, outputs are SHA-256â€™d. |
| **ğŸ¤ Earn Your Submit** | To register a model you must first evaluate *three* other models within 24 hâ€”ensuring the queue is always moving without a central compute budget. |
| **â™»ï¸ Extensible Scoring** | Schema is generic (`Task â†” Benchmark â†” Result`). Future axesâ€”human star-ratings, Green Score (kWh/COâ‚‚), adversarial robustnessâ€”drop in as extra columns & materialized-view math. |
| **ğŸª¤ Minimal Surfacing** | The website does exactly two things: accept signed results & sort them. Everything elseâ€”model hosting, branch history, rich demo cardsâ€”stays on the services that already excel at it. |

### Current MVP Snapshot

* **Backend** : FastAPI + PostgreSQL(+Timescale) + Redis Streams, auto-deployed to Fly.io  
* **Runner** : Typer CLI â†’ Docker runtime (multi-arch), pushes results to API  
* **Benchmarks** : TruthfulQA (5) & GSM8K (5) on HF Datasets  
* **Leaderboard UI** : Next.js 14 (app router) on Vercel, SWR polls every 60 s  
* **CI/CD** : GitHub Actions â€“ pytest + ruff; runtime image to GHCR; backend rolling-deploy

The architecture is intentionally sparse so that future featuresâ€”larger tasks, qualitative reviews, eco-metrics, contributor-driven new benchmarksâ€”slot in with additive migrations and new JWT scopes, *not* rewrites.

> **Mission statement:** *Democratize rapid, accountable, environment-aware evaluation of small-scale LLM tweaksâ€”without re-inventing model hosting or shouldering petabyte bills.*


## 0. TL;DR for Agents ğŸ¤–

1. **Clone** â†’ **`make dev`** â†’ services come up locally.
2. **Pick an Issue** in `docs/ISSUES.md` or open a new one.
3. `git checkout -b feat/<topic>` â†’ code â†’ `pytest -q && ruff .` â†’ `gh pr create`.
4. CI will: lint + unit-test â†’ build runtime image (`ghcr.io/...`) â†’ deploy Backend to Fly â†’ Vercel preview.

Everything else you need (architecture, env-vars, extension hooks, coding conventions) is documented below so an autonomous agent can proceed end-to-end with **zero human context-switch**.

---

## 1. Vision & Non-Goals

|                            |                                                                                                                                                                                             |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Mission**                | Hourly, tamper-proof micro-benchmarks for **HF Hub LLM / LoRA** models that give public credit to contributors.                                                                             |
| **Zero-Storage Principle** | **Binary weights / branches / logs are *never* stored here.** We only keep:<br> â€¢ model URL (optionally `org/repo@sha`)<br> â€¢ numeric scores & metadata<br> â€¢ contributor account + credits |
| **MVP Benchmarks**         | TruthfulQA (5 samples) & GSM8K (5 samples)                                                                                                                                                  |
| **Update cadence**         | Leaderboard refresh on every finished task ( â‰¤ 1 h latency )                                                                                                                                |
| **Î±-phase Exclusions**     | Secure enclaves / ZK proofs, multilingual benchmarks, pretty UI                                                                                                                             |

---

## 2. High-Level System Diagram

```
bench-cli (runner)  <--Redis Streams-->  Backend API (FastAPI)
    |                                           |
    |  Docker eval                               |  PostgreSQL(+Timescale)  <-- materialized view
    |                                           |
 Next.js UI  <-- REST/JSON -->  Backend API  <-- task consensus + cron
```

---

## 3. Current Repository Layout

```
repo/
â”œâ”€ apps/
â”‚  â”œâ”€ backend/         # FastAPI + SQLModel + Redis Streams
â”‚  â”œâ”€ bench-cli/       # Typer CLI & Docker runtime
â”‚  â””â”€ frontend/        # Next.js 14 (app router)
â”œâ”€ alembic/            # DB migrations
â”œâ”€ .github/workflows/  # CI / CD (runtime build, Fly deploy)
â”œâ”€ charts/             # (placeholder) helm manifests
â””â”€ infra/terraform/    # (placeholder) staging IaC
```

<details>
<summary>File-level tree</summary>

```
apps/backend/
  â”œâ”€ auth.py            # JWT scopes, GitHub OIDC exchange
  â”œâ”€ config.py
  â”œâ”€ database.py
  â”œâ”€ main.py            # 5 core endpoints + consensus logic
  â”œâ”€ models.py          # SQLModel ORM
  â”œâ”€ queue.py           # Redis Streams helper
  â”œâ”€ utils.py           # quota calc etc.
  â”œâ”€ Dockerfile
  â””â”€ fly.toml

apps/bench-cli/
  â”œâ”€ main.py            # login / pull / run / submit
  â””â”€ runtime/
      â”œâ”€ Dockerfile     # multi-arch CPU runtime image
      â””â”€ run_eval.py    # TruthfulQA & GSM8K scorer

apps/frontend/
  â”œâ”€ app/               # Next.js pages
  â””â”€ lib/fetcher.ts
```

</details>

---

## 4. Local Development

```bash
# dependencies: docker, python3.11, node18, pnpm (or npm/yarn), uv

# Quick setup with uv (fastest)
make dev             # installs uv, syncs dependencies, sets up environment
source env/dev.sh    # exports environment variables
make migrate         # run database migrations
make run-backend     # start backend server (http://localhost:8000)

# In another terminal
make run-frontend    # start frontend (http://localhost:3000/leaderboard)

# Traditional setup (slower)
uv sync
source env/dev.sh
uv run alembic upgrade head
uv run uvicorn apps.backend.main:app --reload

# Build runtime image
docker build -t bench/runtime:0.1 apps/bench-cli/runtime

# CLI usage (with dummy token for local dev)
export BENCH_API_URL=http://localhost:8000
uv run bench login --token dummy
uv run bench pull --runner-id mygpu --auto
```

### Development Commands

- `make dev` - Setup development environment with uv
- `make run-backend` - Start FastAPI backend server
- `make run-frontend` - Start Next.js frontend
- `make migrate` - Run database migrations
- `make test` - Run test suite
- `make lint` - Run linting and type checking

The project now uses **uv** for blazing-fast Python package management and development.

---

## 5. Runtime & Scoring

* **Data**: sub-sampling of HF Datasets deterministically by SHA.
* **`run_eval.py`** loads model â†’ runs 5 prompts â†’ returns `mean_score`, `runtime_sec`, `stdout_sha`.
* **Energy / COâ‚‚** can be added later by self-reporting `--gpu-type` & power draw; DB already stores runtime seconds.

---

## 6. Consensus & Security

| Layer        | Mechanism                                                        |
| ------------ | ---------------------------------------------------------------- |
| Tamper proof | `dataset_sha`, `stdout_sha`, median of â‰¥3 runs (`max-min < 0.5`) |
| Quorum       | `K_CONSENSUS = 3`, else `DISPUTED` + re-queue                    |
| Isolation    | Docker with `--network none --pids-limit 512`                    |
| AuthN/Z      | GitHub OIDC â†’ JWT `scope` (task\:pull/submit, model\:register)   |
| Quota        | `NORM_REQUIRED = 3` tasks/24 h per submitter                     |

---

## 7. CI / CD

| Workflow               | Trigger                                          | Result                                                                |
| ---------------------- | ------------------------------------------------ | --------------------------------------------------------------------- |
| **docker-runtime.yml** | push to `main`, path `apps/bench-cli/runtime/**` | Build **multi-arch** image â†’ push `ghcr.io/<org>/bench-runtime:<tag>` |
| **fly-deploy.yml**     | push to `main`, path `apps/backend/**`           | `flyctl deploy` backend                                               |
| **ci.yml**             | any push / PR                                    | `pytest + ruff + mypy`                                                |

Secrets needed in repo: `FLY_API_TOKEN`. Fly app secrets: `JWT_SECRET`, `DATABASE_URL`, `REDIS_URL`.

---

## 8. Extensibility Roadmap

| Roadmap item                   | One-liner How-To                                                                                     |
| ------------------------------ | ---------------------------------------------------------------------------------------------------- |
| **Human/Qualitative tasks**    | add `benchmark.type='human'`, route to external label UI, store 1-5â˜… in `TaskResult.score`.          |
| **Green score (energy / COâ‚‚)** | extend `TaskResult` with `energy_kwh`; compute `green_score = f(acc, coâ‚‚)` in MV.                    |
| **Dynamic quota / window**     | env `NORM_REQUIRED`, `NORM_WINDOW_H`; expose `/admin/settings`.                                      |
| **User role escalation**       | materialized view `contributor_level`; cron grants JWT scope `benchmark:create` when thresholds met. |
| **New benchmarks**             | `POST /v0/benchmarks` (scope: `benchmark:create`) â†’ enqueue same task matrix automatically.          |
| **External storage only**      | enforce `model.hf_repo` includes git SHA; keep **no blobs** locally.                                 |

The DB & API surface were intentionally generic so the above need **only column additions and ENV flips**, no rewrite.

---

## 9. Contributing Guidelines (for Humans & Agents)

* **Branch naming**: `feat/<topic>`, `fix/<bug>`, `docs/<area>`.
* **Commits**: conventional-commits (`feat:`, `fix:`, `refactor:` â€¦).
* **Tests**: every PR must add/modify unit tests if logic changes.
* **Lint**: `ruff --fix .` before pushing.
* **PR checklist** (enforced by CI):

  1. Unit tests pass
  2. `ruff` & `mypy` clean
  3. No TODO/FIXME committed
  4. Updated docs / OpenAPI schema

---

## 10. Issue Backlog Snapshot (`docs/ISSUES.md`)

| Pri | Title                           | Area     | Brief                                      |
| --- | ------------------------------- | -------- | ------------------------------------------ |
| P0  | `perf/prefetch-dataset-cache`   | runtime  | warm HF dataset shards in image            |
| P1  | `feat/green-score-plumbing`     | backend  | add energy\_kwh column, ENV `WATT_PER_GPU` |
| P1  | `feat/benchmark-create-endpt`   | backend  | gated by `benchmark:create` scope          |
| P2  | `feat/human-review-portal`      | frontend | minimal crowd UI + Slack/webhook login     |
| P3  | `docs/architecture-diagram.svg` | docs     | nice diagram for README                    |

Agents may autonomously assign themselves (`/assign @agent-name`) and move cards on the GitHub Projects board.

---

### â˜‘ï¸ MVP status: **COMPLETE**

The system runs end-to-end in prod-like conditions with hourly leaderboard, quota gating, CI/CD and no local model storage. Future items fit naturally via additive migrations and scope tweaks.

*This README is intentionally exhaustive so that an LLM-powered agent can onboard, pick issues, and continue development without extra context.*

READMEã®ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã€5ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ã®æ‹¡å¼µæ€§ã«ã¤ã„ã¦æ˜ç¢ºã«ã™ã‚‹å½¢ã§ä¿®æ­£ã—ã¾ã™ï¼š

```markdown

---

## 11. Long-term Roadmap: Towards Community-Driven Model Development

Our MVP intentionally starts minimal, but the vision extends far beyond simple benchmarking. We're building the foundation for a community where model evaluation and development are deeply intertwined.

### Phase 1: Foundation (Current MVP) âœ…
- Basic leaderboard with TruthfulQA & GSM8K micro-benchmarks
- **5-shot evaluation as extensible baseline** (not a limitation, but a starting point)
- Tamper-proof consensus mechanism
- Contributor quota system
- Zero-storage architecture

### Phase 2: Scalable Evaluation Framework (Q2 2025)
- **Dynamic shot configuration**: 5 â†’ 25 â†’ 100+ shots based on task complexity
- **Adaptive evaluation depth**: More shots for close competitions, fewer for clear winners
- **Community-defined evaluation budgets**: Collectively decide compute allocation
- **Progressive evaluation**: Start with 5-shot, expand if model shows promise
- **Evaluation effort remains constant**: Whether quota is 3 or 10, total community workload stays balanced

#### Key Design Principle: Evaluation Elasticity
```yaml
evaluation_depth:
  quick_filter: 5 shots      # MVP: rapid iteration
  standard: 25 shots         # Default for established models
  comprehensive: 100+ shots  # For top-tier models
  custom: community_defined  # Task-specific requirements

# Total community compute remains constant:
# 1000 models Ã— 5 shots = 200 models Ã— 25 shots = 50 models Ã— 100 shots

Phase 3: Evaluation Diversity & Model Repository Integration (Q3 2025)

Human-in-the-loop evaluation: Subjective quality assessments
Multi-modal benchmarks: Image, audio, video comprehension
Domain-specific tracks: Medical, Legal, Code, Scientific reasoning
Commit-level tracking: Link submissions to specific model commits
Training metadata: Attach configs, datasets, compute requirements
Lineage graphs: Visualize model family trees and LoRA inheritance

Phase 4: Community Governance (Q4 2025)

Evaluation DAO: Token-based voting on benchmark selection and shot counts
Contributor tiers:

Evaluators: Run benchmarks, earn credits
Benchmark Authors: Design tasks and define appropriate shot counts
Core Contributors: Define evaluation methodology and resource allocation


Dynamic quota system: Adjust based on current queue and evaluation depth
Compute pooling: Share resources for deeper evaluations of promising models

Phase 5: Evaluation-Driven Development (2026)

Auto-improvement loops: Models fine-tune based on comprehensive evaluation
Tiered evaluation pipeline:
New model â†’ 5-shot screen â†’ 25-shot validation â†’ 100-shot certification

Community consensus on depth: Vote on how deeply to evaluate each model class
Best practices codification: Optimal shot counts for different model types
Impact-weighted evaluation: More shots for models with higher real-world usage

Technical Evolution for Scalable Evaluation
2025 Q1: Fixed 5-shot (MVP baseline)
2025 Q2: Configurable shots (5-100) + batched evaluation
2025 Q3: Adaptive depth based on score variance
2025 Q4: Community-voted evaluation budgets
2026:    ML-optimized shot selection
Evaluation Philosophy
The 5-shot baseline is not a compromise but a design choice for rapid iteration. The architecture supports:

Constant community workload: More models Ã— fewer shots = Fewer models Ã— more shots
Quality over quantity: Better to deeply evaluate important models than superficially evaluate all
Progressive refinement: Start fast, go deep when it matters
Community-driven depth: Let contributors decide where to invest evaluation effort

Getting Involved

Today: Run 5-shot evaluations, help establish baselines
Soon: Propose optimal shot counts for new benchmarks
Future: Vote on community evaluation budget allocation


"The best evaluation depth is not fixedâ€”it emerges from community consensus and available resources."


## ğŸ› ï¸ ãƒ­ãƒ¼ã‚«ãƒ«å‹•ä½œç¢ºèª

ä»¥ä¸‹ã¯ **Ubuntu/macOS, Apple Siliconãƒ»x86\_64 å…±é€š**ã€‚
Windows ã¯ WSL2 ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

### 0. å‰æãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ«                     | Version (ç›®å®‰) | ç”¨é€”                          |
| ----------------------- | ------------ | --------------------------- |
| Docker / Docker Desktop | 24+          | Postgresãƒ»Redisãƒ»runtime ã‚¤ãƒ¡ãƒ¼ã‚¸ |
| Python                  | 3.11         | FastAPI backend / CLI       |
| Node.js                 | â‰¥ 18         | Next.js frontend            |
| pnpm \* or npm/yarn     | æœ€æ–°           | package manager             |

\* **pnpm** ã‚’ä½¿ã†å ´åˆ

```bash
curl -fsSL https://get.pnpm.io/install.sh | sh -
```

---

### 1. ãƒªãƒã‚¸ãƒˆãƒªå–å¾—

```bash
git clone https://github.com/<you>/llm-bench.git
cd llm-bench
```

---

### 2. ã‚¤ãƒ³ãƒ•ãƒ©ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•

#### 2-A. Docker Compose ã§ã¾ã¨ã‚ã¦

```bash
# èµ·å‹•
docker compose -f dev-compose.yml up -d
# æ§‹æˆ:
#   postgres:5432  (user=postgres, pass=pass)
#   redis   :6379
```

<details>
<summary>dev-compose.yml å…¨æ–‡</summary>

```yaml
version: "3.9"
services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_PASSWORD: pass
    ports: ["5432:5432"]

  redis:
    image: redis:7
    ports: ["6379:6379"]
```

</details>

#### 2-B. ç›´æ¥ `docker run` æ´¾ã®æ–¹

```bash
docker run -d --name pg -e POSTGRES_PASSWORD=pass -p 5432:5432 postgres:16
docker run -d --name redis -p 6379:6379 redis:7
```

---

### 3. Python ä»®æƒ³ç’°å¢ƒ & Backend

```bash
python -m venv .venv && source .venv/bin/activate
uv sync
```

#### 3-A. ç’°å¢ƒå¤‰æ•°

```bash
export DATABASE_URL="postgresql+psycopg2://postgres:pass@localhost:5432/postgres"
export REDIS_URL="redis://localhost:6379/0"
export JWT_SECRET="devsecret"               # å¥½ããªæ–‡å­—åˆ—ã§OK
export BENCH_API_URL="http://localhost:8000"
```

`.env` ã‚’ä½œã‚‹å ´åˆ:

```
DATABASE_URL=postgresql+psycopg2://postgres:pass@localhost:5432/postgres
REDIS_URL=redis://localhost:6379/0
JWT_SECRET=devsecret
```

#### 3-B. DB åˆæœŸåŒ–

```bash
alembic upgrade head                 # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
```

ã‚µãƒ³ãƒ—ãƒ«ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æŠ•å…¥ (TruthfulQA, GSM8K):

```bash
python - <<'PY'
from sqlmodel import SQLModel, Session, create_engine
from apps.backend.models import Benchmark
from apps.backend.config import Settings

import os
engine = create_engine(os.environ["DATABASE_URL"])
with Session(engine) as s:
    if not s.exec(SQLModel.select(Benchmark)).all():
        s.add_all([
            Benchmark(name="TruthfulQA", version="0.1", dataset_sha="sha-truth"),
            Benchmark(name="GSM8K",    version="0.1", dataset_sha="sha-gsm"),
        ])
        s.commit()
PY
```

#### 3-C. API èµ·å‹•

```bash
uvicorn apps.backend.main:app --reload --port 8000
```

> OpenAPI ãŒ [http://localhost:8000/docs](http://localhost:8000/docs) ã§ç¢ºèªã§ãã¾ã™ã€‚

---

### 4. Runtime ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰ (ä¸€åº¦ã ã‘)

```bash
docker build -t bench/runtime:0.1 apps/bench-cli/runtime
```

> Apple Silicon ãªã‚‰ `--platform linux/arm64` ã‚’ä»˜ã‘ã¦ã‚‚ OKã€‚

---

### 5. CLI ã‚’ä½¿ç”¨

```bash
uv pip install --system -e apps/bench-cli        # Typer CLI ã‚’ editable-install

# ã€ŒGitHub OIDC ãƒˆãƒ¼ã‚¯ãƒ³ã€ã‚’çœç•¥ã™ã‚‹ä»£ã‚ã‚Šã«æ–‡å­—åˆ— dummy ã‚’é€ã‚‹
bench login --token dummy
```

#### 5-A. ã‚¿ã‚¹ã‚¯ã‚’å¼•ã„ã¦è‡ªå‹•å®Ÿè¡Œ & æå‡º

```bash
# GPU åãªã©ä¸€æ„ãª ID ã‚’ runner-id ã«
watch -n60 'bench pull --runner-id mygpu --auto'
```

*pull â†’ Docker ã§ 5 å•æ¨è«– â†’ result JSON ã‚’ backend ã« POST* ãŒãƒ«ãƒ¼ãƒ—ã—ã¾ã™ã€‚
3 ã‚¿ã‚¹ã‚¯çµ‚ãˆãŸã‚‰ **è‡ªèº«ã®ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ãƒãƒ«ãƒ** ãŒæº€ãŸã•ã‚Œã¾ã™ã€‚

#### 5-B. æ‰‹å‹•ã§è©¦ã™å ´åˆ

```bash
bench pull --runner-id dev           # current_task.json ãŒä¿å­˜
bench run current_task.json          # å®Ÿè¡Œ (ã‚«ãƒ¬ãƒ³ãƒˆã§ out.json ä½œæˆ)
bench submit out.json                # æå‡º
```

---

### 6. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰

```bash
cd apps/frontend
pnpm i                               # npm i / yarn ã§ã‚‚ OK
pnpm dev                             # :3000 ã§èµ·å‹•
# http://localhost:3000/leaderboard ãŒ 60 s ã”ã¨ã«ãƒãƒ¼ãƒªãƒ³ã‚°
```

> **ç’°å¢ƒå¤‰æ•°**
> `NEXT_PUBLIC_API_URL` ã‚’ `http://localhost:8000` ã«ã™ã‚‹ã¨åˆ¥ãƒãƒ¼ãƒˆãƒ»åˆ¥ãƒ›ã‚¹ãƒˆã§ã‚‚å‹•ãã¾ã™ã€‚
> ãƒ­ãƒ¼ã‚«ãƒ«ã¯ `.env.local` ã«æ›¸ãã ã‘ã§ Next.js ãŒæ‹¾ã„ã¾ã™ã€‚

---

## ğŸ‰ å‹•ä½œç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

1. `bench pull --auto` ã®ãƒ­ã‚°ã§

   * `docker run bench/runtime:0.1 ...` ãŒæˆåŠŸ
   * `submitted {'status': 'DONE'}` ãŒè¿”ã‚‹
2. `uvicorn` å´ã®ãƒ­ã‚°ã« `POST /v0/tasks/<id>/result 200` ãŒè¡¨ç¤º
3. `psql` ã§ `SELECT * FROM leaderboard_hourly;` â†’ `avg_score` ãŒå…¥ã£ã¦ã„ã‚‹
4. ãƒ–ãƒ©ã‚¦ã‚¶ `localhost:3000/leaderboard` ã«ãƒ¢ãƒ‡ãƒ«ãŒãƒ©ãƒ³ã‚¯ã‚¤ãƒ³

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

| ç—‡çŠ¶                               | åŸå›  / è§£æ±º                                                                    |
| -------------------------------- | -------------------------------------------------------------------------- |
| `bench pull` ã§ **no task** ã¨å‡ºç¶šã‘ã‚‹ | â‘  ãƒ¢ãƒ‡ãƒ«ã‚’ã¾ã ç™»éŒ²ã—ã¦ã„ãªã„<br>â‘¡ Redis ãŒèµ·å‹•ã—ã¦ã„ãªã„<br>â†’ `docker ps` ã§ redis ã‚³ãƒ³ãƒ†ãƒŠç¢ºèª        |
| `psycopg2.OperationalError`      | `DATABASE_URL` ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒãƒ¼ãƒˆèª¤ã‚Š                                                |
| `CUDA out of memory`             | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ GPU å¯¾å¿œã‚¤ãƒ¡ãƒ¼ã‚¸ã«å·®ã—æ›¿ãˆ or `--gpus all` ã‚’å¤–ã— CPU ã§è©¦ã™                         |
| Leaderboard ãŒæ›´æ–°ã•ã‚Œãªã„              | materialized view ãŒå¤ã„å¯èƒ½æ€§ â†’ `REFRESH MATERIALIZED VIEW leaderboard_hourly;` |

---